{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "778a7951",
   "metadata": {},
   "source": [
    "# Bigdata Engineering - analyzing willhaben data\n",
    "\n",
    "## Scraping the data\n",
    "\n",
    "Willhaben.at, as Austria‚Äôs largest classifieds website, is a rich source of data for market analysis, especially in real estate, automobiles, and second-hand goods. However, scraping Willhaben can be challenging due to dynamic content loading and bot protection. A robust approach involves using Scrapy for structured scraping logic, with Selenium integrated as middleware to render JavaScript and bypass basic anti-bot defenses.\n",
    "\n",
    "### üõ† Tools & Technologies\n",
    "\n",
    "* **Scrapy**: Python-based web crawling framework for structured data extraction.\n",
    "\n",
    "* **Selenium**: Browser automation tool to handle JavaScript-rendered pages.\n",
    "\n",
    "* **Webdriver**: Firefox in headless mode for performance.\n",
    "\n",
    "* **Apache Kafka**: Data streaming pipeline\n",
    "\n",
    "### üîÅ Workflow Overview\n",
    "\n",
    "* Scrapy handles crawl control and item pipelines.\n",
    "\n",
    "* Selenium acts as a renderer for pages that require JS execution.\n",
    "\n",
    "* Middleware intercepts requests needing rendering and returns fully loaded HTML to Scrapy.\n",
    "\n",
    "* Data is extracted using Beautiful soup (XPath/CSS).\n",
    "\n",
    "* After each extraction, the data is forwarded to the Kafka broker\n",
    "\n",
    "### üõ° Anti-Bot Considerations\n",
    "\n",
    "* Random User Agents: Rotate user agents for each session.\n",
    "\n",
    "* Delay & Throttle: Respect server load by implementing DOWNLOAD_DELAY.\n",
    "\n",
    "* Proxy Pools: Use rotating proxies to avoid IP blocks.\n",
    "\n",
    "### üìä Use Cases\n",
    "\n",
    "* Real Estate: Track price changes and availability per district.\n",
    "\n",
    "* Cars: Analyze price depreciation trends by brand and model.\n",
    "\n",
    "* Consumer Goods: Detect high-demand used products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c77a8d",
   "metadata": {},
   "source": [
    "# üï∑ Run the webscraper\n",
    "\n",
    "The first step is to gather the ad urls and willhaben codes from a categorie of your choice.\n",
    "\n",
    "`navigation_url` is the parameter used for iterating through the pages, where the lists of products are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39bd5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isfile('scrapy.cfg'):\n",
    "    os.chdir('willhaben')\n",
    "\n",
    "!scrapy crawl willhaben_urls -a navigation_url=\"https://www.willhaben.at/iad/kaufen-und-verkaufen/marktplatz/buecher/comics-mangas-2913\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd648e9",
   "metadata": {},
   "source": [
    "A new scraping session begins with the creation of a `scrape_run_id`, which is used to reference and track the ads in the subsequent steps.\n",
    "\n",
    "The second phase involves the `willhaben_items` scraper, which retrieves the URLs associated with a given scrape run, sends requests to each ad, and extracts data based on the structure defined in the `WillhabenItem` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae69d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isfile('scrapy.cfg'):\n",
    "    os.chdir('willhaben')\n",
    "\n",
    "!scrapy crawl willhaben_items -a scrape_run_id=13"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd270842",
   "metadata": {},
   "source": [
    "## Example: Analyzing cars üèé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ca5168",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_run_id = 7\n",
    "\n",
    "db_params = {\n",
    "    'dbname': 'scraped',\n",
    "    'user': 'scraped',\n",
    "    'password': 'scraped',\n",
    "    'host': '127.0.0.1',\n",
    "    'port': '5432'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaea572",
   "metadata": {},
   "source": [
    "### Average Mileage vs. Registration Year\n",
    "\n",
    "This script connects to a PostgreSQL database to retrieve car listing data, specifically the registration date (Erstzulassung) and mileage (Kilometerstand) from a JSONB field. After fetching the data related to a specific scrape run, it performs data cleaning and transformation‚Äîparsing dates, converting mileage to numeric values, and filtering out invalid entries. The cleaned data is then grouped by registration year, and the average mileage is calculated for each year. Finally, the processed data is visualized in a line chart, showing trends over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b67920",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "try:\n",
    "    # Establish connection to PostgreSQL database\n",
    "    \n",
    "    connection = psycopg2.connect(**db_params)\n",
    "\n",
    "    # Create a cursor to perform database operations\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define the SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        data->'Basisdaten'->>'Erstzulassung' AS Erstzulassung,\n",
    "        data->'Basisdaten'->>'Kilometerstand' AS Kilometerstand\n",
    "    FROM willhaben_items\n",
    "    WHERE \n",
    "        data->'Basisdaten'->>'Erstzulassung' IS NOT NULL\n",
    "        AND data->'Basisdaten'->>'Kilometerstand' IS NOT NULL\n",
    "        AND scrape_run_id = {scrape_run_id};\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Convert fetched rows to a pandas DataFrame\n",
    "    df = pd.DataFrame(rows, columns=['Erstzulassung', 'Kilometerstand'])\n",
    "\n",
    "    # Data preprocessing\n",
    "    # Convert Erstzulassung to datetime (assuming format like \"4/2018\")\n",
    "    def parse_date(date_str):\n",
    "        try:\n",
    "            return datetime.strptime(date_str, \"%m/%Y\")\n",
    "        except ValueError:\n",
    "            return None  # Handle invalid dates if any\n",
    "\n",
    "    df['Erstzulassung'] = df['Erstzulassung'].apply(parse_date)\n",
    "\n",
    "    # Clean Kilometerstand: remove 'km', replace '.' with '', and convert to numeric\n",
    "    df['Kilometerstand'] = df['Kilometerstand'].str.replace(' km', '').str.replace('.', '').astype(float)\n",
    "\n",
    "    # Drop rows with invalid dates or missing data\n",
    "    df = df.dropna(subset=['Erstzulassung', 'Kilometerstand'])\n",
    "\n",
    "    # Create buckets by extracting the year from Erstzulassung\n",
    "    df['Year'] = df['Erstzulassung'].dt.year\n",
    "\n",
    "    # Aggregate by year: calculate mean Kilometerstand for each year\n",
    "    df_buckets = df.groupby('Year')['Kilometerstand'].mean().reset_index()\n",
    "\n",
    "    # Create a line chart\n",
    "    plt.figure(figsize=(20, 6))\n",
    "    plt.plot(df_buckets['Year'], df_buckets['Kilometerstand'], marker='o', linestyle='-', color='b')\n",
    "    plt.title('Average Mileage vs. Registration Year')\n",
    "    plt.xlabel('Registration Year')\n",
    "    plt.ylabel('Average Mileage')\n",
    "    plt.grid(True)\n",
    "    plt.xticks(df_buckets['Year'], rotation=90)  # Ensure years are shown as integers\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL:\", error)\n",
    "\n",
    "finally:\n",
    "    # Close database connection\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0756c5",
   "metadata": {},
   "source": [
    "### Percentage of Car Colors by Brand\n",
    "\n",
    "This script connects to a PostgreSQL database to extract car listing data, focusing on exterior color (Au√üenfarbe) and brand information derived from the breadcrumbs field. After filtering for valid entries, it constructs brand-color pairs and limits the dataset to the most common brands and colors. A pivot table is then created to count how often each color appears per brand. This table is converted into percentages to normalize the data for comparison. Finally, a stacked bar chart is generated to visualize the color distribution across the top car brands, with custom color mapping to match the actual color names in German."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f018ba9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    # Establish connection to PostgreSQL database\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "\n",
    "    # Create a cursor to perform database operations\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define the SQL query to extract Au√üenfarbe and breadcrumbs\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        data->'Karosserie & Technik'->>'Au√üenfarbe' AS color,\n",
    "        breadcrumbs\n",
    "    FROM willhaben_items\n",
    "    WHERE scrape_run_id = {scrape_run_id};\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Process the colors and brands: collect color-brand pairs\n",
    "    data = []\n",
    "    total_valid_records = 0\n",
    "\n",
    "    for row in rows:\n",
    "        color = row[0]\n",
    "        breadcrumbs = row[1]\n",
    "        if not (color and color.strip() and breadcrumbs):\n",
    "            continue\n",
    "        try:\n",
    "            brand = breadcrumbs.split(',')[3].strip()\n",
    "        except IndexError:\n",
    "            continue\n",
    "        data.append((brand, color.strip()))\n",
    "        total_valid_records += 1\n",
    "\n",
    "    if total_valid_records == 0:\n",
    "        raise ValueError(\"No valid records with non-empty color and brand data found.\")\n",
    "\n",
    "    # Create a DataFrame\n",
    "    df = pd.DataFrame(data, columns=['Brand', 'Color'])\n",
    "\n",
    "    # Limit to top brands and colors\n",
    "    top_brands = df['Brand'].value_counts().head(20).index\n",
    "    top_colors = df['Color'].value_counts().head(20).index\n",
    "    df_filtered = df[df['Brand'].isin(top_brands) & df['Color'].isin(top_colors)]\n",
    "\n",
    "    pivot_table = df_filtered.groupby(['Brand', 'Color']).size().unstack(fill_value=0)\n",
    "\n",
    "    # Calculate percentages for stacking\n",
    "    pivot_table_percentage = pivot_table.div(pivot_table.sum(axis=1), axis=0) * 100\n",
    "    \n",
    "    # Print colors and their counts\n",
    "    color_counts = Counter(df_filtered['Color'])\n",
    "\n",
    "    # Define color mapping (German color names to matplotlib colors)\n",
    "    color_mapping = {\n",
    "        'Schwarz': '#000000',  # Black\n",
    "        'Grau': '#808080',     # Gray\n",
    "        'Wei√ü': '#F5F5F5',     # Light grayish-white (for visibility)\n",
    "        'Blau': '#0000FF',     # Blue\n",
    "        'Silber': '#C0C0C0',   # Silver\n",
    "        'Rot': '#FF0000',      # Red\n",
    "        'Gr√ºn': '#008000',     # Green\n",
    "        'Sonstige': '#D3D3D3', # Light gray\n",
    "        'Gelb': '#FFFF00',     # Yellow\n",
    "        'Braun': '#8B4513',    # Brown\n",
    "        'Orange': '#FFA500',   # Orange\n",
    "        'Violett': '#800080',  # Purple\n",
    "        'Gold': '#FFD700',     # Gold\n",
    "        'Beige': '#F5F5DC',    # Beige\n",
    "        'Bronze': '#CD7F32'    # Bronze\n",
    "    }\n",
    "\n",
    "    # Map colors to the pivot table columns, use fallback for unmapped colors\n",
    "    plot_colors = [color_mapping.get(color, 'lightgray') for color in pivot_table_percentage.columns]\n",
    "\n",
    "    # Create a stacked bar chart with mapped colors\n",
    "    plt.figure(figsize=(12, 8), dpi=100)\n",
    "    ax = pivot_table_percentage.plot(\n",
    "        kind='bar', \n",
    "        stacked=True, \n",
    "        color=plot_colors, \n",
    "        edgecolor='black',  # Add edgecolor for white (Wei√ü) visibility\n",
    "        ax=plt.gca()\n",
    "    )\n",
    "    plt.title('Percentage of Car Colors by Brand', fontsize=10)\n",
    "    plt.xlabel('Brand', fontsize=12)\n",
    "    plt.ylabel('Percentage (%)', fontsize=12)\n",
    "    plt.xticks(fontsize=10, rotation=45, ha='right')\n",
    "    plt.yticks(fontsize=10)\n",
    "    plt.legend(title='Color', labels=pivot_table_percentage.columns, fontsize=14, \n",
    "               bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL:\", error)\n",
    "\n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cf4ad0",
   "metadata": {},
   "source": [
    "### Percentage of Cars with Each Feature (Ausstattungen & Extras) (Top 20)\n",
    "This script connects to a PostgreSQL database to fetch car listing data, specifically focusing on the Ausstattungen & Extras field, which contains a list of car features. After querying the database, the script processes the retrieved JSON data, counting occurrences of each feature across all valid records. It then calculates the percentage of cars that include each feature. The top 50 features are selected, and the results are visualized in a bar chart, showing the percentage of cars that have each feature in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d2948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "try:\n",
    "    # Establish connection to PostgreSQL database\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "\n",
    "    # Create a cursor to perform database operations\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define the SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        data->'Ausstattungen & Extras' AS extras\n",
    "    FROM willhaben_items\n",
    "    WHERE scrape_run_id = {scrape_run_id};\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Process the extras: count features and total valid records\n",
    "    feature_counts = Counter()\n",
    "    total_valid_records = 0\n",
    "\n",
    "    for row in rows:\n",
    "        # Extract the JSONB array (row[0] is the JSONB object)\n",
    "        extras = row[0]\n",
    "        if extras:  # Skip null or empty arrays\n",
    "            try:\n",
    "                # Convert JSONB array to Python list\n",
    "                features = json.loads(json.dumps(extras))\n",
    "                if isinstance(features, list) and features:  # Ensure it's a non-empty list\n",
    "                    feature_counts.update(features)\n",
    "                    total_valid_records += 1  # Count valid records\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue  # Skip invalid JSON or non-list data\n",
    "\n",
    "    # Calculate percentages: (feature count / total valid records) * 100\n",
    "    if total_valid_records == 0:\n",
    "        raise ValueError(\"No valid records with non-empty feature arrays found.\")\n",
    "\n",
    "    feature_percentages = {\n",
    "        feature: (count / total_valid_records) * 100\n",
    "        for feature, count in feature_counts.items()\n",
    "    }\n",
    "\n",
    "    # Convert to a pandas DataFrame\n",
    "    df = pd.DataFrame.from_dict(feature_percentages, orient='index', columns=['Percentage'])\n",
    "    df = df.sort_values('Percentage', ascending=False).reset_index().rename(columns={'index': 'Feature'})\n",
    "\n",
    "    # Limit to top 20 features for readability\n",
    "    df_top = df.head(50)\n",
    "\n",
    "    # Print the DataFrame\n",
    "    print(f\"Feature Percentages (Top 20, based on {total_valid_records} valid records):\")\n",
    "\n",
    "    # Create a bar chart\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(df_top['Feature'], df_top['Percentage'], color='b')\n",
    "    plt.title('Percentage of Cars with Each Feature (Ausstattungen & Extras) (Top 20)')\n",
    "    plt.xlabel('Feature')\n",
    "    plt.ylabel('Percentage of Cars (%)')\n",
    "    plt.xticks(rotation=60, fontsize=10, ha='right')\n",
    "    plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "except (Exception, Error) as error:\n",
    "    print(\"Error while connecting to PostgreSQL:\", error)\n",
    "\n",
    "finally:\n",
    "    # Close database connection\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49a3343",
   "metadata": {},
   "source": [
    "### Median Price vs. Number of Features (Ausstattungen & Extras)\n",
    "\n",
    "This script connects to a PostgreSQL database to fetch car listing data, specifically the features from the Ausstattungen & Extras field and the car price. It processes the data by counting the number of features associated with each car and associates it with the price. The features are grouped into buckets based on their count (e.g., 1-5 features, 6-10 features, etc.), and the median price for each bucket is calculated. Buckets with insufficient data are filtered out. The script then calculates the correlation between the number of features and the median price across the valid buckets. If valid data is present, a plot is generated showing the relationship between the median price and the number of features per car. Finally, the Pearson correlation coefficient between feature count and price is calculated and displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00f0cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    # Establish connection to PostgreSQL database\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "\n",
    "    # Create a cursor to perform database operations\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define the SQL query to fetch extras from JSON and price from column\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        data->'Ausstattungen & Extras' AS extras,\n",
    "        price\n",
    "    FROM willhaben_items\n",
    "    WHERE price IS NOT NULL\n",
    "    AND scrape_run_id = {scrape_run_id};\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "    print(f\"Fetched {len(rows)} rows from the database.\")\n",
    "\n",
    "    # Process the data: count features per car and associate with price\n",
    "    feature_count_price = []\n",
    "    total_valid_records = 0\n",
    "\n",
    "    for row in rows:\n",
    "        extras, price = row[0], row[1]\n",
    "        if extras and price is not None:  # Skip null or empty arrays and null prices\n",
    "            try:\n",
    "                # Convert JSONB array to Python list\n",
    "                features = json.loads(json.dumps(extras))\n",
    "                if isinstance(features, list) and features:  # Ensure it's a non-empty list\n",
    "                    feature_count = len(features)  # Count number of features\n",
    "                    feature_count_price.append((feature_count, price))\n",
    "                    total_valid_records += 1\n",
    "            except (json.JSONDecodeError, TypeError):\n",
    "                continue  # Skip invalid JSON or non-list data\n",
    "\n",
    "    if total_valid_records == 0:\n",
    "        raise ValueError(\"No valid records with non-empty feature arrays and prices found.\")\n",
    "\n",
    "    print(f\"Total valid records: {total_valid_records}\")\n",
    "\n",
    "    # Create a DataFrame from feature counts and prices\n",
    "    df = pd.DataFrame(feature_count_price, columns=['Feature_Count', 'Price'])\n",
    "\n",
    "    # Bucket the feature counts (e.g., 1-5 features, 6-10 features, etc.)\n",
    "    max_features = df['Feature_Count'].max()\n",
    "    bins = range(0, max_features + 5, 5)  # Create buckets of size 5\n",
    "    df['Feature_Bucket'] = pd.cut(df['Feature_Count'], bins=bins, include_lowest=True, right=False)\n",
    "\n",
    "    # Calculate median price per bucket, explicitly setting observed=False\n",
    "    bucket_stats = df.groupby('Feature_Bucket', observed=False).agg(\n",
    "        Median_Price=('Price', 'median'),\n",
    "        Count=('Price', 'count')\n",
    "    ).reset_index()\n",
    "\n",
    "    print(\"\\nRaw bucket statistics before filtering:\")\n",
    "    print(bucket_stats)\n",
    "\n",
    "    # Filter out buckets with insufficient data (e.g., less than 5 cars)\n",
    "    bucket_stats = bucket_stats[bucket_stats['Count'] >= 5]\n",
    "\n",
    "    print(\"\\nFiltered bucket statistics (Count >= 5):\")\n",
    "    print(bucket_stats)\n",
    "\n",
    "    # Extract bucket midpoints and median prices for correlation\n",
    "    bucket_midpoints = [float(interval.mid) for interval in bucket_stats['Feature_Bucket']]  # Convert Interval to float\n",
    "    median_prices = bucket_stats['Median_Price'].astype(float)  # Ensure numeric type\n",
    "\n",
    "    print(\"\\nBucket midpoints:\", bucket_midpoints)\n",
    "    print(\"Median prices:\", median_prices.tolist())\n",
    "\n",
    "    # Calculate Pearson correlation coefficient if sufficient valid data\n",
    "    if (len(bucket_midpoints) > 1 and\n",
    "        not median_prices.isna().any() and\n",
    "        all(isinstance(x, (int, float)) for x in bucket_midpoints)):\n",
    "        correlation = np.corrcoef(bucket_midpoints, median_prices)[0, 1]\n",
    "    else:\n",
    "        correlation = np.nan\n",
    "        print(\"Correlation not calculated: Insufficient valid data or invalid types.\")\n",
    "\n",
    "    # Print results\n",
    "    print(f\"\\nCorrelation between Feature Count and Median Price: {correlation:.2f}\")\n",
    "    print(\"\\nBucket Statistics:\")\n",
    "    print(bucket_stats)\n",
    "\n",
    "    # Create a plot if there is valid data\n",
    "    if bucket_midpoints and not median_prices.isna().any():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(bucket_midpoints, median_prices, 'bo-', label=f'Correlation: {correlation:.2f}')\n",
    "        plt.title('Median Price vs. Number of Features (Ausstattungen & Extras)')\n",
    "        plt.xlabel('Number of Features (Bucket Midpoint)')\n",
    "        plt.ylabel('Median Price (‚Ç¨)')\n",
    "        plt.grid(True, linestyle='--', alpha=0.7)\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Plot not generated: No valid data for plotting.\")\n",
    "\n",
    "except Exception as error:\n",
    "    print(f\"An unexpected error occurred: {error}\")\n",
    "\n",
    "finally:\n",
    "    # Close database connection\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d60c3",
   "metadata": {},
   "source": [
    "### Getting coordinates based on contact address\n",
    "\n",
    "This script connects to a PostgreSQL database, fetches addresses from the willhaben_items table, and uses the Nominatim geocoder from the geopy library to convert those addresses into geographic coordinates (latitude and longitude). For each valid geocoded address, it updates the corresponding database record with the calculated coordinates. The script also tracks the geocoding results, storing the coordinates for potential analysis or visualization. It includes error handling for invalid addresses and other geocoding issues, and respects the rate limit of the Nominatim API by optionally pausing between requests. After processing, the script closes the database connection to ensure proper resource management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31399c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import numpy as np\n",
    "from geopy.geocoders import Nominatim\n",
    "\n",
    "import time\n",
    "\n",
    "# Initialize Nominatim geocoder\n",
    "geolocator = Nominatim(user_agent=\"myGeocoder\")\n",
    "\n",
    "try:\n",
    "    # Establish connection to PostgreSQL database\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "\n",
    "    # Create a cursor to perform database operations\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define the SQL query to fetch willhaben_code and contact_address\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        willhaben_code,\n",
    "        contact_address\n",
    "    FROM willhaben_items\n",
    "    WHERE contact_address IS NOT NULL\n",
    "    AND scrape_run_id = {scrape_run_id};\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "    print(f\"Fetched {len(rows)} rows from the database.\")\n",
    "\n",
    "    # Lists to store geocoded coordinates\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    addresses = []\n",
    "    codes = []\n",
    "\n",
    "    # Geocode each address and update the table\n",
    "    for row in rows:\n",
    "        willhaben_code, contact_address = row\n",
    "        try:\n",
    "            # Clean and geocode the address\n",
    "            location = geolocator.geocode(contact_address, timeout=10)\n",
    "            if location:\n",
    "                # Update the coordinates column with \"latitude,longitude\"\n",
    "                cursor.execute(\"\"\"\n",
    "                    UPDATE willhaben_items\n",
    "                    SET coordinates = %s\n",
    "                    WHERE willhaben_code = %s;\n",
    "                \"\"\", (f\"{location.latitude},{location.longitude}\", willhaben_code))\n",
    "\n",
    "                connection.commit()\n",
    "                # Store for plotting\n",
    "                latitudes.append(location.latitude)\n",
    "                longitudes.append(location.longitude)\n",
    "                addresses.append(location.address)\n",
    "                codes.append(willhaben_code)\n",
    "                print(f\"Geocoded and updated: {contact_address} -> ({location.latitude}, {location.longitude})\")\n",
    "            else:\n",
    "                print(f\"Failed to geocode: {contact_address}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error geocoding {contact_address}: {e}\")\n",
    "        # Respect Nominatim's rate limit (1 request per second)\n",
    "        #time.sleep(1)\n",
    "\n",
    "except Exception as error:\n",
    "    print(f\"An unexpected error occurred: {error}\")\n",
    "\n",
    "finally:\n",
    "    # Close database connections\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Database connection closed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec3e64",
   "metadata": {},
   "source": [
    "### Visualization of coordinates\n",
    "\n",
    "This script connects to a PostgreSQL database and fetches coordinates, willhaben_code, and contact_address from the willhaben_items table where coordinates are available. It processes the rows by extracting latitude and longitude from the coordinates field, handling any parsing errors, and stores the data in lists for further use. After gathering the coordinates, it uses Plotly to generate a density heatmap, visualizing the distribution of locations based on the latitude and longitude values. The map is centered around the average coordinates, and the heatmap uses a color scale to represent the density of listings. Finally, the script displays the heatmap, providing an insightful view of the data's geographical spread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6088cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from psycopg2 import Error\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import display\n",
    "\n",
    "# Database connection parameters\n",
    "db_params = {\n",
    "    'dbname': 'scraped',\n",
    "    'user': 'scraped',\n",
    "    'password': 'scraped',\n",
    "    'host': '127.0.0.1',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Establish connection to PostgreSQL database\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "    print(\"Successfully connected to PostgreSQL database.\")\n",
    "\n",
    "    # Create a cursor to perform database operations\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define the SQL query to fetch coordinates, willhaben_code, and contact_address\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        willhaben_code,\n",
    "        contact_address,\n",
    "        coordinates\n",
    "    FROM willhaben_items\n",
    "    WHERE coordinates IS NOT NULL\n",
    "    AND scrape_run_id = {scrape_run_id};\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query\n",
    "    cursor.execute(query)\n",
    "\n",
    "    # Fetch all rows\n",
    "    rows = cursor.fetchall()\n",
    "    print(f\"Fetched {len(rows)} rows from the database.\")\n",
    "\n",
    "    # Lists to store data for plotting\n",
    "    latitudes = []\n",
    "    longitudes = []\n",
    "    codes = []\n",
    "    addresses = []\n",
    "\n",
    "    # Process rows (for coordinates column)\n",
    "    for row in rows:\n",
    "        willhaben_code, contact_address, coordinates = row\n",
    "        try:\n",
    "            # Parse coordinates from \"latitude,longitude\"\n",
    "            lat, lon = map(float, coordinates.split(','))\n",
    "            latitudes.append(lat)\n",
    "            longitudes.append(lon)\n",
    "            codes.append(willhaben_code)\n",
    "            addresses.append(contact_address)\n",
    "        except ValueError as e:\n",
    "            print(f\"Invalid coordinates format for {willhaben_code}: {coordinates}\")\n",
    "\n",
    "    # Alternative: Process rows for PostGIS (uncomment if using geom)\n",
    "    \"\"\"\n",
    "    for row in rows:\n",
    "        willhaben_code, contact_address, latitude, longitude = row\n",
    "        latitudes.append(latitude)\n",
    "        longitudes.append(longitude)\n",
    "        codes.append(willhaben_code)\n",
    "        addresses.append(contact_address)\n",
    "    \"\"\"\n",
    "\n",
    "except Exception as error:\n",
    "    print(f\"An unexpected error occurred: {error}\")\n",
    "\n",
    "finally:\n",
    "    # Close database connections\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()\n",
    "        print(\"Database connection closed.\")\n",
    "\n",
    "# Create density heatmap\n",
    "if latitudes and longitudes:\n",
    "    fig = go.Figure(go.Densitymap(\n",
    "        lat=latitudes,\n",
    "        lon=longitudes,\n",
    "        radius=20,  # Adjust for density spread\n",
    "        opacity=0.7,\n",
    "        zauto=True,\n",
    "        colorscale='Hot',\n",
    "        showscale=True,\n",
    "        text=[f\"{code}: {addr}\" for code, addr in zip(codes, addresses)]\n",
    "    ))\n",
    "\n",
    "    # Update layout for map\n",
    "    fig.update_layout(\n",
    "        title='Willhaben Anzeigen Density Heatmap',\n",
    "        mapbox=dict(\n",
    "            style='open-street-map',  # Free map style\n",
    "            center=dict(\n",
    "                lat=np.mean(latitudes),\n",
    "                lon=np.mean(longitudes)\n",
    "            ),\n",
    "            zoom=1\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, t=30, b=0)\n",
    "    )\n",
    "\n",
    "    # Display the plot\n",
    "    display(fig)\n",
    "else:\n",
    "    print(\"No coordinates available to plot.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20efde89",
   "metadata": {},
   "source": [
    "### Prepare data for Kafka streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1faf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_basisdaten(basis_str):\n",
    "    try:\n",
    "        return json.loads(basis_str)\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        return {}\n",
    "\n",
    "def extract_kw(leistung_str):\n",
    "    if isinstance(leistung_str, str):\n",
    "        match = re.search(r'(\\d+)\\s*kW', leistung_str)\n",
    "        if match:\n",
    "            return int(match.group(1))\n",
    "    return None\n",
    "\n",
    "def parse_erstzulassung(date_str):\n",
    "    try:\n",
    "        return pd.to_datetime(date_str, format='%m/%Y')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "def clean_kilometer(km_str):\n",
    "    if isinstance(km_str, str):\n",
    "        digits = re.sub(r'[^\\d]', '', km_str)\n",
    "        return int(digits) if digits else None\n",
    "    return None\n",
    "\n",
    "try:\n",
    "    # Connect to PostgreSQL\n",
    "    connection = psycopg2.connect(**db_params)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Define and run the SQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        willhaben_code,\n",
    "        title,\n",
    "        price,\n",
    "        breadcrumbs,\n",
    "        data->>'Basisdaten' AS basisdaten\n",
    "    FROM willhaben_items\n",
    "    WHERE scrape_run_id = {scrape_run_id}\n",
    "      AND data IS NOT NULL\n",
    "      AND breadcrumbs IS NOT NULL\n",
    "    \"\"\"\n",
    "    cursor.execute(query)\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    # Define column names\n",
    "    columns = ['willhaben_code', 'title', 'price', 'breadcrumbs', 'basisdaten']\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(rows, columns=columns)\n",
    "\n",
    "    # Extract brand and model\n",
    "    df[['brand', 'model']] = df['breadcrumbs'].str.split(',', expand=True).iloc[:, -2:]\n",
    "    df = df.drop(columns=['breadcrumbs'])\n",
    "    df['price'] = df['price'].astype(float)\n",
    "\n",
    "    # Expand basisdaten into columns\n",
    "    basisdaten_expanded = df['basisdaten'].apply(parse_basisdaten).apply(pd.Series)\n",
    "    df = pd.concat([df, basisdaten_expanded], axis=1)\n",
    "\n",
    "    # Clean extracted fields\n",
    "    df['Leistung'] = df['Leistung'].apply(extract_kw)\n",
    "    \n",
    "    df['Erstzulassung'] = df['Erstzulassung'].apply(lambda x: datetime.strptime(x, \"%m/%Y\").strftime(\"%Y-%m\") if isinstance(x, str) else None)\n",
    "    \n",
    "    df['Kilometerstand'] = df['Kilometerstand'].apply(clean_kilometer)\n",
    "\n",
    "    # Drop raw basisdaten column\n",
    "    df = df.drop(columns=['basisdaten'])\n",
    "\n",
    "    # Convert DataFrame to list of dicts\n",
    "    json_data = df.to_dict(orient='records')\n",
    "    \n",
    "finally:\n",
    "    if connection:\n",
    "        cursor.close()\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb8b6f1",
   "metadata": {},
   "source": [
    "#### Send data to Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e8c3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Kafka config\n",
    "KAFKA_BROKER = '172.29.16.101:9092'\n",
    "KAFKA_TOPIC = 'willhaben-items'\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=KAFKA_BROKER,\n",
    "    value_serializer=lambda v: json.dumps(v, ensure_ascii=False).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Send each JSON object to Kafka\n",
    "for record in json_data:\n",
    "    producer.send(KAFKA_TOPIC, value=record)\n",
    "\n",
    "producer.flush()\n",
    "print(f\"Sent {len(json_data)} messages to Kafka topic '{KAFKA_TOPIC}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac56f1de",
   "metadata": {},
   "source": [
    "### Map car brand production countries from external CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aeccda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the brand‚Äìcountry mapping CSV\n",
    "brand_country_df = pd.read_csv(\"country_by_carbrands.csv\")\n",
    "\n",
    "# Merge to add the country based on the brand\n",
    "df = df.merge(brand_country_df, how='left', left_on='brand', right_on='Brand')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata-scrapy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
